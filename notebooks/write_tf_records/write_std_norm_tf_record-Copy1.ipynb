{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdee5c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 00:22:04.719754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:22:04.759710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:22:04.760145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# I think the below should be commented out\n",
    "# for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import deepcell\n",
    "# Changed from before due to new placement of Track, concat_tracks\n",
    "from deepcell_tracking.utils import load_trks\n",
    "from deepcell.data.tracking import Track, concat_tracks\n",
    "##############\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepcell.utils.data_utils import reshape_movie\n",
    "from deepcell.utils.transform_utils import erode_edges\n",
    "from deepcell.data import split_dataset\n",
    "from deepcell_toolbox.processing import normalize, histogram_normalization\n",
    "\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2af084",
   "metadata": {},
   "source": [
    "# Load dictionaries of usable (non-blank, non-border) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2c3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_img_dict(file):\n",
    "    f = open(file)\n",
    "    d = json.load(f)\n",
    "    d = {int(k1): {int(k2): {int(k3): v for k3, v in d[k1][k2].items()} for k2, d[k1][k2] in d[k1].items()} for k1, d[k1] in d.items()}\n",
    "    return d\n",
    "def load_img_idx_dict(file):\n",
    "    f = open(file)\n",
    "    d = json.load(f)\n",
    "    d = {int(k): v for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bf7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of the form {batch: {cell: {frame: -1/idx, ...}, ...}, ...} where good_imgs[batch][cell][frame] = -1 if the image\n",
    "# is blank/on the border and idx if it is usable, where idx is the position in the list of usable images\n",
    "train_good_imgs = load_img_dict('/data/dataset_pruning/train_appearances_dict.json')\n",
    "train_blank_imgs = load_img_dict('/data/dataset_pruning/train_blank_dict.json')\n",
    "train_border_imgs = load_img_dict('/data/dataset_pruning/train_border_dict.json')\n",
    "val_good_imgs = load_img_dict('/data/dataset_pruning/val_appearances_dict.json')\n",
    "val_blank_imgs = load_img_dict('/data/dataset_pruning/val_blank_dict.json')\n",
    "val_border_imgs = load_img_dict('/data/dataset_pruning/val_border_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f5b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lst_idx_to_pos(good_imgs_dict):\n",
    "    lst_idx_to_pos = {}\n",
    "    for b in good_imgs_dict.keys():\n",
    "        for c in good_imgs_dict[b].keys():\n",
    "            for f in good_imgs_dict[b][c].keys():\n",
    "                idx = good_imgs_dict[b][c][f]\n",
    "                if idx != -1:\n",
    "                    lst_idx_to_pos = (b, f, c)\n",
    "    return lst_idx_to_pos\n",
    "\n",
    "# Dictionary of frames to split (time until splitting) for each image, in the format of usable image dictionary\n",
    "def get_life_cycle_dict(tracks):\n",
    "    batches, frames, cells = tracks.centroids.shape[:3]\n",
    "    \n",
    "    life_cycle_dict = {}\n",
    "    for batch in range(batches):\n",
    "        life_cycle_dict[batch] = {}\n",
    "        for cell in range(cells):\n",
    "            life_cycle_dict[batch][cell] = {}\n",
    "            for frame in range(frames):\n",
    "                life_cycle_dict[batch][cell][frame] = -1\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        for cell in tracks.lineages[batch].keys():\n",
    "            if len(tracks.lineages[batch][cell]['daughters']) != 0:\n",
    "                cell_frames = tracks.lineages[batch][cell]['frames']\n",
    "                last_frame = cell_frames[-1]\n",
    "                for cell_frame in cell_frames:\n",
    "                    life_cycle_dict[batch][cell - 1][cell_frame] = last_frame - cell_frame + 1\n",
    "    \n",
    "    return life_cycle_dict\n",
    "\n",
    "# Should the cells in the dictionary be 0-indexed? What is the case in the good_imgs_dict?\n",
    "# Yes. It is 0-indexed wrt the cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b1f45",
   "metadata": {},
   "source": [
    "# Write appearances array to TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa652aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.io import serialize_tensor\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.backend import is_sparse\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        # BytesList won't unpack a string from an EagerTensor.\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array\n",
    "\n",
    "def create_tracking_example(image):\n",
    "    # define the dictionary -- the structure -- of our single example\n",
    "    data = {\n",
    "        'height' : _int64_feature(image.shape[0]),\n",
    "        'width' : _int64_feature(image.shape[1]),\n",
    "        'depth' : _int64_feature(image.shape[2]),\n",
    "        'raw_image' : _bytes_feature(serialize_array(image))\n",
    "    }\n",
    "    # create an Example, wrapping the single features\n",
    "    out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f31355ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No (extra) normalization\n",
    "def write_tracking_dataset_to_tfr(track, filename, good_imgs):\n",
    "    print('Done calculating normalizations.')\n",
    "    filename = \"/data/tf_records/\" + filename + \".tfrecords\"\n",
    "    writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our data to disk\n",
    "    count = 0\n",
    "\n",
    "    app = track.appearances\n",
    "    # Only write usable images to the record\n",
    "    for b in range(app.shape[0]):\n",
    "        print(b)\n",
    "        for f in range(app.shape[1]):\n",
    "            for c in range(app.shape[2]):\n",
    "                # Check if the given image is usable\n",
    "                if good_imgs[b][c][f] != -1:\n",
    "                    current_image = app[b, f, c]\n",
    "\n",
    "                    out = create_tracking_example(current_image)\n",
    "\n",
    "                    if out is not None:\n",
    "                        writer.write(out.SerializeToString())\n",
    "                        count += 1\n",
    "\n",
    "    writer.close()\n",
    "    print(f'Wrote {count} elements to TFRecord')\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9107ebe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_trks = load_trks(os.path.join('/data', 'train.trks'))\n",
    "# val_trks = load_trks(os.path.join('/data', 'val.trks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91099331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def norm(X, size):\n",
    "    X_norm = histogram_normalization(X, kernel_size=size)\n",
    "    \n",
    "    X_rescaled = np.zeros(X_norm.shape)\n",
    "    for img_idx in range(X.shape[0]):\n",
    "        x = X[img_idx]\n",
    "        x = rescale_intensity(x, out_range=(-0.5, 0.5))\n",
    "        X_rescaled[img_idx] = x\n",
    "            \n",
    "    return X_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da4b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done calculating normalizations.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "Wrote 89436 elements to TFRecord\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    val_tracks = Track(tracked_data=val_trks,\n",
    "                   appearance_dim=64,\n",
    "                   distance_threshold=64,\n",
    "                   crop_mode='fixed')\n",
    "\n",
    "    print('Done creating Track objects.')\n",
    "\n",
    "    write_tracking_dataset_to_tfr(val_tracks, filename='val_big_norm_contracted', good_imgs=val_good_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e186c303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 00:24:51.025439: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 00:24:51.027731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:24:51.028004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:24:51.028172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:24:52.052940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:24:52.053240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:24:52.053483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-20 00:24:52.054239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46722 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:05:00.0, compute capability: 8.6\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [05:11<00:00,  3.42s/it]\n",
      " 33%|███████████████████████████████████████████████████████                                                                                                                | 30/91 [03:19<07:34,  7.45s/it]/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:264: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:256: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 91/91 [07:57<00:00,  5.25s/it]\n",
      "2022-12-20 00:39:21.245749: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5645002080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with creating Track objects.\n",
      "Done calculating normalizations.\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "Wrote 383800 elements to TFRecord\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_tracks = Track(tracked_data=train_trks,\n",
    "                   appearance_dim=64,\n",
    "                   distance_threshold=64,\n",
    "                   crop_mode='fixed')\n",
    "\n",
    "    print('Done with creating Track objects.')\n",
    "\n",
    "    write_tracking_dataset_to_tfr(train_tracks, filename='train_big_norm_contracted', good_imgs=train_good_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ddf2e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_tracks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_life_cycle_dict \u001b[38;5;241m=\u001b[39m get_life_cycle_dict(\u001b[43mtrain_tracks\u001b[49m)\n\u001b[1;32m      2\u001b[0m val_life_cycle_dict \u001b[38;5;241m=\u001b[39m get_life_cycle_dict(val_tracks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_tracks' is not defined"
     ]
    }
   ],
   "source": [
    "train_life_cycle_dict = get_life_cycle_dict(train_tracks)\n",
    "val_life_cycle_dict = get_life_cycle_dict(val_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890cbb3",
   "metadata": {},
   "source": [
    "# Load images from TF Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edf965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfr_element(element):\n",
    "    #use the same structure as above; it's kinda an outline of the structure we now want to create\n",
    "    data = {\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width':tf.io.FixedLenFeature([], tf.int64),\n",
    "        'depth':tf.io.FixedLenFeature([], tf.int64),\n",
    "        'raw_image' : tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "\n",
    "    height = content['height']\n",
    "    width = content['width']\n",
    "    depth = content['depth']\n",
    "    raw_image = content['raw_image']\n",
    "    \n",
    "\n",
    "    #get our 'feature'-- our image -- and reshape it appropriately\n",
    "    feature = tf.io.parse_tensor(raw_image, out_type=tf.float32)\n",
    "    feature = tf.reshape(feature, shape=[height,width,depth])\n",
    "    return (feature, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34053119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filename, batch_size=1, buffer_size=256,\n",
    "                    seed=None):\n",
    "    #create the dataset\n",
    "    dataset = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "    #pass every single feature through our mapping function\n",
    "    dataset = dataset.map(\n",
    "      parse_tfr_element\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.shuffle(buffer_size, seed=seed)\n",
    "\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "    \n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25daa6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = get_dataset(\"/data/tf_records/val_big_norm_contracted.tfrecords\", batch_size=100)\n",
    "train_dataset = get_dataset(\"/data/tf_records/train_big_norm_contracted.tfrecords\", batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "026880e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbf24d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    \n",
    "    def __init__(self, dim_z, kl_weight, learning_rate, n_filters, n_layers, side):\n",
    "        # change dim from (28, 28, 1)\n",
    "        self.dim_x = (side, side, 1)\n",
    "        self.dim_z = dim_z\n",
    "        self.kl_weight = kl_weight\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_filters = n_filters\n",
    "        self.n_layers = n_layers\n",
    "        self.side = side\n",
    "\n",
    "    # Sequential API encoder\n",
    "    def encoder_z(self):\n",
    "        # define prior distribution for the code, which is an isotropic Gaussian\n",
    "        prior = tfd.Independent(tfd.Normal(loc=tf.zeros(self.dim_z), scale=1.), \n",
    "                                reinterpreted_batch_ndims=1)\n",
    "        # build layers argument for tfk.Sequential()\n",
    "        input_shape = self.dim_x\n",
    "        layers = [tfkl.InputLayer(input_shape=input_shape)]\n",
    "        for i in range(self.n_layers):\n",
    "            layers.append(tfkl.Conv2D(filters=self.n_filters, kernel_size=3, strides=(2,2), \n",
    "                                  padding='valid', activation='relu'))\n",
    "        layers.append(tfkl.Flatten())\n",
    "        # the following two lines set the output to be a probabilistic distribution\n",
    "        layers.append(tfkl.Dense(tfpl.IndependentNormal.params_size(self.dim_z), \n",
    "                                 activation=None, name='z_params'))\n",
    "        layers.append(tfpl.IndependentNormal(self.dim_z, \n",
    "            convert_to_tensor_fn=tfd.Distribution.sample, \n",
    "            activity_regularizer=tfpl.KLDivergenceRegularizer(prior, weight=self.kl_weight), \n",
    "            name='z_layer'))\n",
    "        return tfk.Sequential(layers, name='encoder')\n",
    "    \n",
    "    # Sequential API decoder\n",
    "    def decoder_x(self):\n",
    "        layers = [tfkl.InputLayer(input_shape=self.dim_z)]\n",
    "        # probably 7 before since 28/2/2 = 7, so changing to 32/2/2 = 8\n",
    "        frac = 2**self.n_layers\n",
    "        layers.append(tfkl.Dense(int((self.side/frac)**2*32), activation=None))\n",
    "        layers.append(tfkl.Reshape((int(self.side/frac),int(self.side/frac),32)))\n",
    "        for i in range(self.n_layers):\n",
    "            layers.append(tfkl.Conv2DTranspose(filters=self.n_filters, kernel_size=3, strides=2, \n",
    "                                           padding='same', activation='relu'))\n",
    "        layers.append(tfkl.Conv2DTranspose(filters=1, kernel_size=3, strides=1, \n",
    "                                           padding='same'))\n",
    "        layers.append(tfkl.Flatten())\n",
    "        # note that here we don't need \n",
    "        # `tfkl.Dense(tfpl.IndependentBernoulli.params_size(self.dim_x))` because \n",
    "        # we've restored the desired input shape with the last Conv2DTranspose layer\n",
    "        layers.append(tfkl.Dense(tfpl.IndependentNormal.params_size(self.dim_x), \n",
    "                                 activation=None, name='x_params'))\n",
    "        layers.append(tfpl.IndependentNormal(self.dim_x,\n",
    "            name='x_layer'))\n",
    "        return tfk.Sequential(layers, name='decoder')\n",
    "    \n",
    "    def build_vae_keras_model(self):\n",
    "        x_input = tfk.Input(shape=self.dim_x)\n",
    "        encoder = self.encoder_z()\n",
    "        decoder = self.decoder_x()\n",
    "        z = encoder(x_input)\n",
    "\n",
    "        # compile VAE model\n",
    "        model = tfk.Model(inputs=x_input, outputs=decoder(z))\n",
    "        model.compile(loss=negative_log_likelihood, \n",
    "                      optimizer=tfk.optimizers.Adam(self.learning_rate))\n",
    "        return model\n",
    "\n",
    "# the negative of log-likelihood for probabilistic output\n",
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "534105d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 07:31:03.999210: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# VAE with 4 encoding and decoding layers, 64 filters per convolution, KL-divergence weight 1\n",
    "# dim_z, kl_weight, learning_rate, n_filters, n_layers, side\n",
    "vae = VAE(16, 1, 1e-3, 64, 4, 64)\n",
    "AE = vae.build_vae_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "071a6bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 07:33:36.273134: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2022-12-20 07:33:39.257454: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3838/3838 [==============================] - 86s 21ms/step - loss: nan - val_loss: nan - lr: 0.0010\n",
      "Epoch 2/8\n",
      " 377/3838 [=>............................] - ETA: 1:04 - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model_path = '/data/models/big_img_4_layers_16_embed_10_beta'\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     11\u001b[0m       \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#         save_weights_only=True)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mAE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_callbacks\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "from deepcell import train_utils\n",
    "\n",
    "steps_per_epoch = 3838\n",
    "validation_steps = 895\n",
    "n_epochs = 8\n",
    "# model_path = '/data/models/big_img_4_layers_16_embed_10_beta'\n",
    "\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "      \n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)#,\n",
    "#     tf.keras.callbacks.ModelCheckpoint(\n",
    "#         model_path, monitor='val_loss',\n",
    "#         save_best_only=True, verbose=1,\n",
    "#         save_weights_only=True)\n",
    "]\n",
    "\n",
    "loss_history = AE.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75fa6a2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PrefetchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m      4\u001b[0m xhat \u001b[38;5;241m=\u001b[39m AE(x)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      6\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PrefetchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([train_dataset[0][0][0]])\n",
    "xhat = AE(x).mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(x[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(xhat[0])\n",
    "\n",
    "print(x[0, 0, 0])\n",
    "print(x[0, 30, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2018ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = get_dataset(\"/data/tf_records/val_big.tfrecords\", batch_size=100)\n",
    "train_dataset = get_dataset(\"/data/tf_records/train_big.tfrecords\", batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3fd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE with 4 encoding and decoding layers, 64 filters per convolution, KL-divergence weight 1\n",
    "# dim_z, kl_weight, learning_rate, n_filters, n_layers, side\n",
    "vae2 = VAE(16, 1, 1e-3, 64, 4, 64)\n",
    "AE2 = vae.build_vae_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "774b7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "3838/3838 [==============================] - 154s 40ms/step - loss: -5496.5293 - val_loss: 5227.3755 - lr: 0.0010\n",
      "Epoch 2/4\n",
      "3838/3838 [==============================] - 154s 40ms/step - loss: -6287.4819 - val_loss: 570.2474 - lr: 0.0010\n",
      "Epoch 3/4\n",
      "3838/3838 [==============================] - 153s 40ms/step - loss: -6690.2344 - val_loss: -1890.8573 - lr: 0.0010\n",
      "Epoch 4/4\n",
      "3838/3838 [==============================] - 153s 40ms/step - loss: -5318.7632 - val_loss: -2278.5364 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.optimizers import RectifiedAdam as RAdam\n",
    "from deepcell import train_utils\n",
    "\n",
    "steps_per_epoch = 3838\n",
    "validation_steps = 895\n",
    "n_epochs = 4\n",
    "# model_path = '/data/models/big_img_4_layers_16_embed_10_beta'\n",
    "\n",
    "train_callbacks = [\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "      \n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)#,\n",
    "#     tf.keras.callbacks.ModelCheckpoint(\n",
    "#         model_path, monitor='val_loss',\n",
    "#         save_best_only=True, verbose=1,\n",
    "#         save_weights_only=True)\n",
    "]\n",
    "\n",
    "loss_history = AE2.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=train_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "785cde1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3e600dcbe0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([val_tracks.appearances[2][13][6]])\n",
    "xhat = AE2(x).mean()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(x[0])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(xhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee4daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
