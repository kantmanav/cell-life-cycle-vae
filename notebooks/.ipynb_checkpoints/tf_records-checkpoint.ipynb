{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3692cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import deepcell\n",
    "# Changed from before due to new placement of Track, concat_tracks\n",
    "from deepcell_tracking.utils import load_trks\n",
    "from deepcell.data.tracking import Track, concat_tracks\n",
    "##############\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepcell.utils.data_utils import reshape_movie\n",
    "from deepcell.utils.transform_utils import erode_edges\n",
    "from deepcell.data import split_dataset\n",
    "from deepcell_toolbox.processing import normalize, histogram_normalization\n",
    "\n",
    "import spektral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df81ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_img_dict(file):\n",
    "    f = open(file)\n",
    "    d = json.load(f)\n",
    "    d = {int(k1): {int(k2): {int(k3): v for k3, v in d[k1][k2].items()} for k2, d[k1][k2] in d[k1].items()} for k1, d[k1] in d.items()}\n",
    "    return d\n",
    "def load_img_idx_dict(file):\n",
    "    f = open(file)\n",
    "    d = json.load(f)\n",
    "    d = {int(k): v for k, v in d.items()}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1dd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_good_imgs = load_img_dict('../dataset_pruning/train_appearances_dict.json')\n",
    "train_blank_imgs = load_img_dict('../dataset_pruning/train_blank_dict.json')\n",
    "train_border_imgs = load_img_dict('../dataset_pruning/train_border_dict.json')\n",
    "val_good_imgs = load_img_dict('../dataset_pruning/val_appearances_dict.json')\n",
    "val_blank_imgs = load_img_dict('../dataset_pruning/val_blank_dict.json')\n",
    "val_border_imgs = load_img_dict('../dataset_pruning/val_border_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the images are written to disk one at a time, we can modify the write function to write the image to disk\n",
    "# only when the image is \"good.\" Can pass in the dictionary of good images as an argument. The funciton uses \n",
    "# track.appearances, the NP array, to write the file. Since the dictionary corresponds with the indices of the \n",
    "# array, we can use this to determine which images to add.\n",
    "\n",
    "# Based on this line in the Track class, 'appearances = np.zeros(batch_shape + appearance_shape, dtype='float32')',\n",
    "# it seems the track.appearances object does not have any batch/cells/frames pattern, but we should check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f575ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7426fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ad79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepcell_tracking.trk_io import load_trks\n",
    "from deepcell_tracking.utils import get_max_cells\n",
    "from deepcell.data.tracking import Track\n",
    "# Might want to import this just to get the functions it uses\n",
    "from deepcell.utils.tfrecord_utils import write_tracking_dataset_to_tfr\n",
    "\n",
    "def get_arg_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--data-path',\n",
    "                        default='/training/tracking-nuclear',\n",
    "                        help='Path to the training data.')\n",
    "\n",
    "    parser.add_argument('--appearance-dim', type=int, default=64)\n",
    "    parser.add_argument('--distance-threshold', type=int, default=64)\n",
    "    parser.add_argument('--crop-mode', type=str, default='fixed')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tracking_dataset_to_tfr(track,\n",
    "                                  filename,\n",
    "                                  good_imgs,\n",
    "                                  target_max_cells=168,\n",
    "                                  verbose=True):\n",
    "\n",
    "    filename_tfr = filename + '.tfrecord'\n",
    "    filename_csv = filename + '.csv'\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(filename_tfr)\n",
    "\n",
    "    # Get features to add\n",
    "    # WE PROBABLY ONLY CARE ABOUT APP\n",
    "    app = track.appearances\n",
    "\n",
    "    # Pad cells - we need to do this to use validation data\n",
    "    # during training\n",
    "\n",
    "    # TARGET MAX CELLS WILL BE THE MAXIMUM 'CELLS' DIMENSION BETWEEN TRAIN AND VAL,\n",
    "    # AND THE OTHER ONE WILL BE PADDED TO ACHIEVE THAT. I PROBABLY DON'T NEED TO\n",
    "    # PAD ANYTHING, SINCE MY DATA IS IN THE FORMAT (num_imgs, dim, dim, 1), NOT\n",
    "    # (batches, frames, cells, dim, dim, 1).\n",
    "    \n",
    "    # Iterate over all batches\n",
    "    # THIS SHOULD PROBABLY BE THE CELLS THEMSELVES IN MY CASE\n",
    "    for b in range(app.shape[0]):\n",
    "        for f in range(app.shape[1]):\n",
    "            for c in range(app.shape[2]):\n",
    "                if good_imgs[b][f][c] != -1:\n",
    "                    img = app[b, f, c]\n",
    "                    track_dict = {'app': img}\n",
    "\n",
    "                    example = create_tracking_example(track_dict)\n",
    "\n",
    "                    if example is not None:\n",
    "                        writer.write(example.SerializeToString())\n",
    "                        count += 1\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Wrote {count} elements to TFRecord')\n",
    "\n",
    "    # WE'LL WORRY ABOUT THE CSV WRITER FOR METADATA AT THE END, IF NECESSARY\n",
    "    # OKAY WE MIGHT NEED IT TO PARSE THE DATA\n",
    "    # Save dataset metadata\n",
    "    # THIS SHOULD BE OKAY--WE JUST HAVE ONE KEY RATHER THAN A BUNCH, NOW\n",
    "    dataset_keys = track_dict.keys()\n",
    "    dataset_dims = [len(track_dict[k].shape) for k in dataset_keys]\n",
    "\n",
    "    with open(filename_csv, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        rows = [[k, dims] for k, dims in zip(dataset_keys, dataset_dims)]\n",
    "        writer.writerows(rows)\n",
    "        \n",
    "        # SHOULDN'T NEED ROWS FOR adj_shape AND temp_adj_shape, SINCE WE'RE NOT\n",
    "        # WRITING THESE\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33c42eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably no reaon to use argument parser--only there because they wanted to run it from the command line\n",
    "# args = get_arg_parser().parse_args([])\n",
    "\n",
    "# train_trks = load_trks(os.path.join(args.data_path, 'train.trks'))\n",
    "val_trks = load_trks(os.path.join('/training/tracking-nuclear', 'val.trks'))\n",
    "\n",
    "# max_cells = max([get_max_cells(train_trks['y']), get_max_cells(val_trks['y'])])\n",
    "\n",
    "# for split, trks in zip({'train', 'val'}, [train_trks, val_trks]):\n",
    "#     print('Preparing {} as tf record'.format(split))\n",
    "\n",
    "#     with tf.device('/cpu:0'):\n",
    "#         tracks = Track(tracked_data=trks,\n",
    "#                        appearance_dim=args.appearance_dim,\n",
    "#                        distance_threshold=args.distance_threshold,\n",
    "#                        crop_mode=args.crop_mode)\n",
    "\n",
    "#         write_tracking_dataset_to_tfr(tracks, target_max_cells=max_cells, filename=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1abe5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 27/27 [02:50<00:00,  6.30s/it]\n",
      "100%|███████████████████████████████████████████| 27/27 [03:51<00:00,  8.57s/it]\n",
      "2022-08-05 00:29:38.413052: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-05 00:29:39.427545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10415 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:09:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "val_tracks = Track(tracked_data=val_trks, appearance_dim=32, distance_threshold=64, crop_mode='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a770fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 71, 277, 32, 32, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tracks.appearances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858f744a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mget_arg_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_trks \u001b[38;5;241m=\u001b[39m load_trks(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.trks\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m val_trks \u001b[38;5;241m=\u001b[39m load_trks(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval.trks\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.8/argparse.py:1783\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[1;32m   1782\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1783\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/usr/lib/python3.8/argparse.py:2533\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   2532\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2533\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/argparse.py:2520\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m   2519\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2520\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "# MEM % dropped heavily (at least from 70 to 40 when elements written to TF Record)\n",
    "# And again when val written to TF Records (down to 14% at the end)\n",
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c400c401",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mtracks\u001b[49m\u001b[38;5;241m.\u001b[39mappearances)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tracks' is not defined"
     ]
    }
   ],
   "source": [
    "type(tracks.appearances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fb2690",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_trks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_tracks \u001b[38;5;241m=\u001b[39m Track(tracked_data\u001b[38;5;241m=\u001b[39m\u001b[43mval_trks\u001b[49m, appearance_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mappearance_dim, distance_threshold\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdistance_threshold, crop_mode\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mcrop_mode)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_trks' is not defined"
     ]
    }
   ],
   "source": [
    "val_tracks = Track(tracked_data=val_trks, appearance_dim=args.appearance_dim, distance_threshold=args.distance_threshold, crop_mode=args.crop_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc0a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नमस्ते। मैं अब बिहारी यंत्र बन गया हूँ।\n"
     ]
    }
   ],
   "source": [
    "print('नमस्ते। मैं अब बिहारी यंत्र बन गया हूँ।')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tracking_example(example, dataset_ndims,\n",
    "                           dtype=tf.float32):\n",
    "    \"\"\"Parse a tracking example\n",
    "    Args:\n",
    "        example (tf.train.Example): The tracking example to be parsed\n",
    "        dataset_ndims (dict): Dictionary of dataset metadata\n",
    "        dtype (tf dtype): Dtype of training data\n",
    "    \"\"\"\n",
    "    # WE MIGHT NEED THE METADATA NOW, TO PARSE IT\n",
    "\n",
    "    # WHAT IS THE DIFFERENCE BETWEEN X AND y?\n",
    "    X_names = ['app', 'cent', 'morph', 'adj']\n",
    "    y_names = ['temp_adj']\n",
    "\n",
    "    sparse_names = ['adj', 'temp_adj']\n",
    "\n",
    "    full_name_dict = {'app': 'appearances',\n",
    "                      'cent': 'centroids',\n",
    "                      'morph': 'morphologies',\n",
    "                      'adj': 'adj_matrices',\n",
    "                      'temp_adj': 'temporal_adj_matrices'}\n",
    "\n",
    "    # Recreate the example structure\n",
    "    data = {}\n",
    "    shape_strings_dict = {}\n",
    "    shapes_dict = {}\n",
    "\n",
    "    for key in dataset_ndims:\n",
    "        if 'shape' in key:\n",
    "            new_key = '_'.join(key.split('_')[0:-1])\n",
    "            shapes_dict[new_key] = dataset_ndims[key]\n",
    "\n",
    "    for key in shapes_dict:\n",
    "        dataset_ndims.pop('{}_shape'.format(key))\n",
    "\n",
    "    for key in dataset_ndims:\n",
    "        if key in sparse_names:\n",
    "            data[key] = tf.io.SparseFeature(value_key='{}_val'.format(key),\n",
    "                                            index_key=['{}_ind_{}'.format(key, i)\n",
    "                                                       for i in range(dataset_ndims[key])],\n",
    "                                            size=shapes_dict[key],\n",
    "                                            dtype=tf.float32)\n",
    "        else:\n",
    "            data[key] = tf.io.FixedLenFeature([], tf.string)\n",
    "\n",
    "        shape_strings = ['{}_shape_{}'.format(key, i)\n",
    "                         for i in range(dataset_ndims[key])]\n",
    "        shape_strings_dict[key] = shape_strings\n",
    "\n",
    "        for ss in shape_strings:\n",
    "            data[ss] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "    # Get data\n",
    "    content = tf.io.parse_single_example(example, data)\n",
    "\n",
    "    X_dict = {}\n",
    "    y_dict = {}\n",
    "\n",
    "    for key in dataset_ndims:\n",
    "\n",
    "        # Get the feature and reshape\n",
    "        if key in sparse_names:\n",
    "            value = content[key]\n",
    "        else:\n",
    "            shape = [content[ss] for ss in shape_strings_dict[key]]\n",
    "            value = content[key]\n",
    "            value = tf.io.parse_tensor(value, out_type=dtype)\n",
    "            value = tf.reshape(value, shape=shape)\n",
    "\n",
    "        if key in X_names:\n",
    "            X_dict[full_name_dict[key]] = value\n",
    "        else:\n",
    "            y_dict[full_name_dict[key]] = value\n",
    "\n",
    "    return X_dict, y_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
